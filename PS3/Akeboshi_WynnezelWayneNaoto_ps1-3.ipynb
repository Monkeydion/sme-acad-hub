{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Computational Intelligence in Manufacturing Systems**\n",
    "#### **MFAIMFG**\n",
    "###### Created by: Wynnezel Wayne Naoto P Akeboshi\n",
    "##### Checked by: SME Academics Database team\n",
    "##### Initial Publish: January 10, 2021\n",
    "##### Assignment Code from the class of: Dr. Robert Kerwin Billones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Machine Learning Exercise 3 - Multi-class Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **INTRODUCTION** (Give a short background about the topic)\n",
    "\n",
    "Multiclass Classification is a classification task with more than two classifications. [1] For example, instead of a binary category such as dogs and cats, we have more than 2 classifications like dogs, cats, and birds. But in this method, a primary assumption is made such that each item fits in one and only one classification. For example, should we classify based on ethnicities: Native American, Hispanic, Asian, individuals (observations) that fit in both Asian and Hispanic or Native American and Hispanic must be excluded. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **CODE DESIGN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Code segment no. 1**\n",
    "*Numpy* is a library primarily used for advanced mathematical operations specifically in linear algebra and matrix operations. [2]\n",
    "\n",
    "*Pandas* is a dataset handling library that reads and writes multiple formats of data files such as .csv, .mat, .txt, and .json files. Pandas, built on numpy, is used primarily for handling, cleaning, manipulating, and presenting data in a tabular form. [3]\n",
    "\n",
    "*matplotlib.pyplot* is a sub-library within matplotlib that is used primarily for plotting datasets in a MatLab-esque kind of way. This allows for simple data visualization during data analysis. [4]\n",
    "\n",
    "*scipy.io* is the scipy input and output library that primary deals with reading different types of files to be read within a python program. [5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__header__': b'MATLAB 5.0 MAT-file, Platform: GLNXA64, Created on: Sun Oct 16 13:09:09 2011',\n",
       " '__version__': '1.0',\n",
       " '__globals__': [],\n",
       " 'X': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
       " 'y': array([[10],\n",
       "        [10],\n",
       "        [10],\n",
       "        ...,\n",
       "        [ 9],\n",
       "        [ 9],\n",
       "        [ 9]], dtype=uint8)}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "%matplotlib inline\n",
    "\n",
    "data = loadmat('ex3data1.mat')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Code segment no. 2** \n",
    "In this segment, we identify the shape (or sizes) of our two arrays. This segment is essential for matrix operations and data training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 400), (5000, 1))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['X'].shape, data['y'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Code segment no. 3**\n",
    "Creates the sigmoid function taking a parameter z. Since the sigmoid function is the backbone of the learning algorithm used in this exercise and will be repeatedly used, it is more efficient to create a function for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Code segment no. 4**\n",
    "The cost function is a function that measures the performance of a Machine Learning model by quantifying the error between the predicted value and the expected value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(theta, X, y, learningRate):\n",
    "    theta = np.matrix(theta)\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y)\n",
    "    first = np.multiply(-y, np.log(sigmoid(X * theta.T)))\n",
    "    second = np.multiply((1 - y), np.log(1 - sigmoid(X * theta.T)))\n",
    "    reg = (learningRate / 2 * len(X)) * np.sum(np.power(theta[:,1:theta.shape[1]], 2))\n",
    "    return np.sum(first - second) / (len(X)) + reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Code segment no. 5** \n",
    "This code defines the gradient function. The gradient function is a optimization function that measures how much the output of a function changes when changing inputs. The loop in this function is necessary to go through each gradient one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_with_loop(theta, X, y, learningRate):\n",
    "    theta = np.matrix(theta)\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y)\n",
    "    \n",
    "    parameters = int(theta.ravel().shape[1])\n",
    "    grad = np.zeros(parameters)\n",
    "    \n",
    "    error = sigmoid(X * theta.T) - y\n",
    "    \n",
    "    for i in range(parameters):\n",
    "        term = np.multiply(error, X[:,i])\n",
    "        \n",
    "        if (i == 0):\n",
    "            grad[i] = np.sum(term) / len(X)\n",
    "        else:\n",
    "            grad[i] = (np.sum(term) / len(X)) + ((learningRate / len(X)) * theta[:,i])\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Code segment no. 6**\n",
    "This defines the gradient function. In this code segment, the for loop is omitted. Instead, using linear algebra, matrix operations were used to remove the need of going through each item in the arrays one by one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(theta, X, y, learningRate):\n",
    "    theta = np.matrix(theta)\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y)\n",
    "    \n",
    "    parameters = int(theta.ravel().shape[1])\n",
    "    error = sigmoid(X * theta.T) - y\n",
    "    \n",
    "    grad = ((X.T * error) / len(X)).T + ((learningRate / len(X)) * theta)\n",
    "    \n",
    "    # intercept gradient is not regularized\n",
    "    grad[0, 0] = np.sum(np.multiply(error, X[:,0])) / len(X)\n",
    "    \n",
    "    return np.array(grad).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Code segment no. 7**\n",
    "Utilizing the scipy sub-library optimize, the entirity of cost minimization and data processing was combined into a single function. Instead of the previous methods of necessitating the use of a cost function and the gradient function, in one function, the entirety of the machine learning model was handled and combined in this function, ultimately returning the final theta needed for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def one_vs_all(X, y, num_labels, learning_rate):\n",
    "    rows = X.shape[0]\n",
    "    params = X.shape[1]\n",
    "    \n",
    "    # k X (n + 1) array for the parameters of each of the k classifiers\n",
    "    all_theta = np.zeros((num_labels, params + 1))\n",
    "    \n",
    "    # insert a column of ones at the beginning for the intercept term\n",
    "    X = np.insert(X, 0, values=np.ones(rows), axis=1)\n",
    "    \n",
    "    # labels are 1-indexed instead of 0-indexed\n",
    "    for i in range(1, num_labels + 1):\n",
    "        theta = np.zeros(params + 1)\n",
    "        y_i = np.array([1 if label == i else 0 for label in y])\n",
    "        y_i = np.reshape(y_i, (rows, 1))\n",
    "        \n",
    "        # minimize the objective function\n",
    "        fmin = minimize(fun=cost, x0=theta, args=(X, y_i, learning_rate), method='TNC', jac=gradient)\n",
    "        all_theta[i-1,:] = fmin.x\n",
    "    \n",
    "    return all_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Code segment no. 8** \n",
    "This code segment is mostly preparation. The rows, parameters, all_theta, and theta are prepared here to be inputted in the functions later. These steps are taken to utilize variables X, y, theta, and all_theta. After preparation of variables, the shapes of these data are identify to ensure that the matrix operations later on are allowed. \n",
    "\n",
    "Note that all the previous code segments only *defined* functions. There has been no data manipulation yet with the actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 401), (5000, 1), (401,), (10, 401))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = data['X'].shape[0]\n",
    "params = data['X'].shape[1]\n",
    "\n",
    "all_theta = np.zeros((10, params + 1))\n",
    "\n",
    "X = np.insert(data['X'], 0, values=np.ones(rows), axis=1)\n",
    "\n",
    "theta = np.zeros(params + 1)\n",
    "\n",
    "y_0 = np.array([1 if label == 0 else 0 for label in data['y']])\n",
    "y_0 = np.reshape(y_0, (rows, 1))\n",
    "\n",
    "X.shape, y_0.shape, theta.shape, all_theta.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Code segment no. 9** \n",
    "np.unique returns the *unique* values within an array. In this case, the unique values that can be found in data['y'], which is the output arrray, are all integers from 1 to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  4  5  6  7  8  9 10]\n",
      "No. of Labels:  10\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(data['y']))\n",
    "print('No. of Labels: ', len(np.unique(data['y'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Code segment no. 10**\n",
    "All the theta (coefficients/weights that fit the model), are generated here using the one_vs_all function previously defined in segment 7. We enter our X input data, y output data, the number of labels (10 as found in code segment 9), and a learning rate (which is completely up to you).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.70247933e-05,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "        -2.24803591e-10,  2.31962901e-11,  0.00000000e+00],\n",
       "       [-8.96250738e-05,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         7.26120841e-09, -6.19965312e-10,  0.00000000e+00],\n",
       "       [-8.39553367e-05,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "        -7.61695604e-10,  4.64917644e-11,  0.00000000e+00],\n",
       "       ...,\n",
       "       [-7.00832571e-05,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "        -6.92009196e-10,  4.29241582e-11,  0.00000000e+00],\n",
       "       [-7.65187952e-05,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "        -8.09503301e-10,  5.31058734e-11,  0.00000000e+00],\n",
       "       [-6.63412433e-05,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "        -3.49765885e-09,  1.13668536e-10,  0.00000000e+00]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_theta = one_vs_all(data['X'], data['y'], 10, 1)\n",
    "all_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Code segment no. 11**\n",
    "We define the prediction function in this code segment. We compute the probability of each entry in X by multiplying it to all_theta, to show the probability that an entry is within a certain class. For example, entry 1 has a probability of 0.4 to be classification A, 0.6 to be classification B, and 0.55 to be classification C. Using the np.argmax, this returns the indeces (the classification) with the highest probability. As seen in the example, this would return index 1 (0-based index) for entry 1. Thus classifying entry 1 to be in classification B.\n",
    "\n",
    "This function eventually returns an array containing the index classifications of each entry, completing its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_all(X, all_theta):\n",
    "    rows = X.shape[0]\n",
    "    params = X.shape[1]\n",
    "    num_labels = all_theta.shape[0]\n",
    "    \n",
    "    # same as before, insert ones to match the shape\n",
    "    X = np.insert(X, 0, values=np.ones(rows), axis=1)\n",
    "    \n",
    "    # convert to matrices\n",
    "    X = np.matrix(X)\n",
    "    all_theta = np.matrix(all_theta)\n",
    "    \n",
    "    # compute the class probability for each class on each training instance\n",
    "    h = sigmoid(X * all_theta.T)\n",
    "    \n",
    "    # create array of the index with the maximum probability\n",
    "    h_argmax = np.argmax(h, axis=1)\n",
    "    \n",
    "    # because our array was zero-indexed we need to add one for the true label prediction\n",
    "    h_argmax = h_argmax + 1\n",
    "    \n",
    "    return h_argmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Code segment no. 12**\n",
    "Using the predict_all function from segment 11, it takes all the input values within data['X'] and the optimized all_theta (weights or coefficients of the model) to predict classifications of entries in data['X'] and be saved to array y_pred.\n",
    "\n",
    "The correct predictions are then counted and the accuracy is computed using this score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 74.6%\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict_all(data['X'], all_theta)\n",
    "correct = [1 if a == b else 0 for (a, b) in zip(y_pred, data['y'])]\n",
    "accuracy = (sum(map(int, correct)) / float(len(correct)))\n",
    "print('accuracy = {0}%'.format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **ANALYSIS OF RESULTS**\n",
    "In this exercise, we displayed different methods of coding the training models. In code segments 5, 6, and 7, different ways of computing for the thetas were shown from most brute force to most efficient, utilizing useful libraries in the process. We can see the efficiency in the coding process by using code segment 7, function one_vs_all, despite it simply being the exact same process as code segments 5 and 6. All of which, though not displayed in the code, would have resulted in the same model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **CONCLUSION**\n",
    "In conclusion, utilizing more libraries and coding in a more efficient manner is possible. Nearly every modelling process may have a library with a single function doing the entire process. But it is essential for us to understand the backbone of these libraries as to see the nuances wherein we should or should not use such automated processes. Ultimately, the multi-class classification model resulted in a 74.6% accuracy, which would have been the same regardless of the use of additional 'shortcut' libraries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **References**\n",
    "[1] https://towardsdatascience.com/machine-learning-multiclass-classification-with-imbalanced-data-set-29f6a177c1a </br>\n",
    "[2] https://www.geeksforgeeks.org/python-numpy/ </br>\n",
    "[3] https://www.learnpython.org/en/Pandas_Basics </br>\n",
    "[4] https://matplotlib.org/3.3.3/api/_as_gen/matplotlib.pyplot.html </br>\n",
    "[5] https://docs.scipy.org/doc/scipy/reference/io.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
